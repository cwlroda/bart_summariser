Most of these parameters are explained in more detail in `this blog post
<https://huggingface.co/blog/how-to-generate>`__.

Parameters:

input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
    The sequence used as a prompt for the generation. If :obj:`None` the method initializes
    it as an empty :obj:`torch.LongTensor` of shape :obj:`(1,)`.

decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
    initial input_ids for the decoder of encoder-decoder type models. If :obj:`None` then only
    decoder_start_token_id is passed as the first token to the decoder.


max_length (:obj:`int`, `optional`, defaults to 20):
    The maximum length of the sequence to be generated.

min_length (:obj:`int`, `optional`, defaults to 10):
    The minimum length of the sequence to be generated.

do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether or not to use sampling ; use greedy decoding otherwise.

early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.

num_beams (:obj:`int`, `optional`, defaults to 1):
    Number of beams for beam search. 1 means no beam search.

temperature (:obj:`float`, `optional`, defaults tp 1.0):
    The value used to module the next token probabilities.

top_k (:obj:`int`, `optional`, defaults to 50):
    The number of highest probability vocabulary tokens to keep for top-k-filtering.

top_p (:obj:`float`, `optional`, defaults to 1.0):
    If set to float < 1, only the most probable tokens with probabilities that add up to ``top_p`` or
    higher are kept for generation.

repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):
    The parameter for repetition penalty. 1.0 means no penalty. See `this paper
    <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.

pad_token_id (:obj:`int`, `optional`):
    The id of the `padding` token.

bos_token_id (:obj:`int`, `optional`):
    The id of the `beginning-of-sequence` token.

eos_token_id (:obj:`int`, `optional`):
    The id of the `end-of-sequence` token.

length_penalty (:obj:`float`, `optional`, defaults to 1.0):
    Exponential penalty to the length. 1.0 means no penalty.

    Set to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in
    order to encourage the model to produce longer sequences.

no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):
    If set to int > 0, all ngrams of that size can only occur once.

bad_words_ids(:obj:`List[int]`, `optional`):
    List of token ids that are not allowed to be generated. In order to get the tokens of the words that
    should not appear in the generated text, use :obj:`tokenizer.encode(bad_word, add_prefix_space=True)`.

num_return_sequences(:obj:`int`, `optional`, defaults to 1):
    The number of independently computed returned sequences for each element in the batch.

attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
    Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for
    tokens that are not masked, and 0 for masked tokens.

    If not provided, will default to a tensor the same shape as :obj:`input_ids` that masks the pad token.

    `What are attention masks? <../glossary.html#attention-mask>`__

decoder_start_token_id (:obj:`int`, `optional`):
    If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.

use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):
    Whether or not the model should use the past last key/values attentions (if applicable to the model) to
    speed up decoding.

model_kwargs:
    Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model.